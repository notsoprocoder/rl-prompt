{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaacbowers-barnard/Documents/Projects/llm-adverse-rl-agent/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/isaacbowers-barnard/Documents/Projects/llm-adverse-rl-agent/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaacbowers-barnard/Documents/Projects/llm-adverse-rl-agent/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/Users/isaacbowers-barnard/Documents/Projects/llm-adverse-rl-agent/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10       |\n",
      "|    ep_rew_mean     | 55.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 1        |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1397     |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2b08536d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "from rl_prompt_injection.environment import ToxicityEnvironment\n",
    "from rl_prompt_injection.engines import action_engine, reward_engine, state_engine\n",
    "from initial_experiment_config import DataConstants, InitialExperimentConstants, BasicConfig\n",
    "\n",
    "\n",
    "\n",
    "llm = pipeline(\n",
    "    \"text2text-generation\", model=ExperimentConstants.MODEL\n",
    "    )\n",
    "def generate_response(s: str | list, model=llm) -> str: \n",
    "    return model(s)[0].get(\"generated_text\", \"\")\n",
    "\n",
    "\n",
    "reward_eng = reward_engine.ResponseToxicityRewardEngine()\n",
    "action_eng = action_engine.Text2TextActionSpace(model=MODEL, num_actions=ExperimentConstants.NUM_ACTIONS)\n",
    "state_eng = state_engine.SentenceTransformerStateEngine()\n",
    "text = pd.read_csv(DataConstants.TRAIN_DATA_PATH, encoding=\"latin1\").iloc[:,-1].dropna().to_numpy()\n",
    "env = ToxicityEnvironment(\n",
    "        llm = generate_response,\n",
    "        reward_engine=reward_eng,\n",
    "        state_engine=state_eng,\n",
    "        action_engine=action_eng,\n",
    "        instruction_prompt = ExperimentConstants.INSTRUCTION_PROMPT,\n",
    "        texts = text,\n",
    "        log_interval=BasicConfig.LOG_INTERVAL,\n",
    "        experiement_dir=ExperimentConstants.OUTPUT_DIR\n",
    ")\n",
    "\n",
    "\n",
    "check_env(env,skip_render_check=True)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=ExperimentConstants.TIMESTEPS)\n",
    "\n",
    "# log final elements\n",
    "env.log()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add experiments\n",
    "# run experiments\n",
    "# what does this tell us? LLM vs Toxicity Model\n",
    "# commentry: feasibility (inaccess but could imagine foundation model service)\n",
    "# statistical test for model robustness\n",
    "# Other experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
